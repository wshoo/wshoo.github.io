<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>scrapy on wshoo的博客</title>
    <link>https://wshoo.github.io/tags/scrapy/</link>
    <description>Recent content in scrapy on wshoo的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sun, 13 Jan 2019 06:18:30 +0800</lastBuildDate>
    
	<atom:link href="https://wshoo.github.io/tags/scrapy/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>scrapy--proxy代理</title>
      <link>https://wshoo.github.io/post/scrapy_proxy/</link>
      <pubDate>Sun, 13 Jan 2019 06:18:30 +0800</pubDate>
      
      <guid>https://wshoo.github.io/post/scrapy_proxy/</guid>
      <description>scrapy&amp;ndash;proxy代理的设置 在构造的请求中添加meta属性： 示例：
def start_requests(self): yield scrapy.FormRequest(url=self.start_urls[0], headers=self.header, formdata=self.data, callback=self.parse_page, meta={&#39;proxy&#39;:&#39;http://1.2.3.4:11&#39;}, )  显然在有多个spider时这种方法不够灵活
以中间件的形式添加 在middlewares.py中添加：
import random, base64 class ProxyMiddleware(object): proxyList = [&#39;36.250.69.4:80&#39;, &#39;58.18.52.168:3128&#39;, &#39;58.253.238.243:80&#39;, &#39;60.191.164.22:3128&#39;, &#39;60.191.167.93:3128&#39;] def process_request(self, request, spider): # Set the location of the proxy pro_adr = random.choice(self.proxyList) print(&amp;quot;USE PROXY -&amp;gt; &amp;quot;+pro_adr) request.meta[&#39;proxy&#39;] = &amp;quot;http://&amp;quot;+ pro_adr &#39;&#39;&#39;这里用的免费代理,不用用户名密码的.如果有用户名和密码,还要加入以下代码 proxy_user_pass = &amp;quot;USERNAME:PASSWORD&amp;quot; encoded_user_pass = base64.encodestring(proxy_user_pass) request.headers[&#39;Proxy-Authorization&#39;] = &#39;Basic &#39; + encoded_user_pass&#39;&#39;&#39;  settings.py:
DOWNLOADER_MIDDLEWARES = { #项目名称改成自己的 &#39;pro_name.</description>
    </item>
    
    <item>
      <title>scrapy--模拟登陆</title>
      <link>https://wshoo.github.io/post/scrapy_login/</link>
      <pubDate>Sun, 13 Jan 2019 06:18:30 +0800</pubDate>
      
      <guid>https://wshoo.github.io/post/scrapy_login/</guid>
      <description>scrapy&amp;ndash;模拟登陆 cookie  HTTP是无状态的协议，为了保持连接状态，引入cookie机制。 cookie 是http消息头的一种属性， 包括： - cookie名字（name）cookie的值（value） - cookie的过期时间（Expires/Max-Age） - cookie作用路径（Path） - cookie所在的域名（Domian）,使用cookie进行安全连接（Secure） - cookie大小Size 其中前两个为必须条件
 表单 使用的Chrome自带的开发者工具中的Network功能，在登录界面输入错误的信息，得到Form Data，下面为知乎的登录信息:
_xsrf: fawfasdfgasdfgadsf email: 11@22.com password: 1123123 rememberme: y  其中_xsrf是一种验证机制，可以从登录界面的源代码中找到。
scrapy模拟登陆 示例：
def start_requests(self): return [Request(&amp;quot;https://www.zhihu.com/login&amp;quot;, callback = self.post_login)] #重写了爬虫类的方法, 实现了自定义请求, 运行成功后会调用callback回调函数 #FormRequeset def post_login(self, response): print &#39;Preparing login&#39; #下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字, 用于成功提交表单 xsrf = Selector(response).xpath(&#39;//input[@name=&amp;quot;_xsrf&amp;quot;]/@value&#39;).extract()[0] print xsrf #FormRequeset.from_response是Scrapy提供的一个函数, 用于post表单 #登陆成功后, 会调用after_login回调函数 return [FormRequest.from_response(response, formdata = { &#39;_xsrf&#39;: xsrf, &#39;email&#39;: &#39;123456&#39;, &#39;password&#39;: &#39;123456&#39; }, callback = self.</description>
    </item>
    
    <item>
      <title>scrapy--proxy代理</title>
      <link>https://wshoo.github.io/post/scrapy_workflow/</link>
      <pubDate>Sun, 13 Jan 2019 05:18:30 +0800</pubDate>
      
      <guid>https://wshoo.github.io/post/scrapy_workflow/</guid>
      <description> scrapy工作流程  Spiders(爬虫):它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器) Engine(引擎)：负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。 Scheduler(调度器)：它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。 Downloader(下载器)：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理 ItemPipeline(管道):它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方. Downloader Middlewares（下载中间件）：你可以当作是一个可以自定义扩展下载功能的组件。 Spider Middlewares（Spider中间件）：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）  </description>
    </item>
    
    <item>
      <title>scrapy 中的 LinkExtractor</title>
      <link>https://wshoo.github.io/post/linkextractor/</link>
      <pubDate>Sun, 18 Nov 2018 06:18:30 +0800</pubDate>
      
      <guid>https://wshoo.github.io/post/linkextractor/</guid>
      <description>LinkExtractor 作用 提取链接
用法实例 from scrapy.linkextractors import LinkExtractor def parse(self, response): le = LinkExtractor(resttrict_xpath=&#39;//ul&#39;) links = le.extract_links(response) if links: next_url = link[0].url yiled scrapy.Request(next_url, callback=self.parse)  LinkExtractor参数  allow 接收一个正则表达式或正则式列表 deny 与allow相反
pattern = &#39;/intro/.+\.html$&#39; le = LinkExtractor(allow=pattern)  allow_domains 接收一个域名或一个域名列表
 deny_domains 与allow_domains相反
domains = [&#39;github.com&#39;, &#39;google.com&#39;] le = LinkExtractor(allow_domains=domains)  restrict_xpaths 接收一个XPath表达式或一个XPath表达式列表
 restrict_css 接收一个CSS选择器或一个CSS选择器列表
le = LinkExtractor(restrict_xpaths=&#39;//div[@id=&amp;quot;top&amp;quot;]&#39;)  tags 接收一个标签（字符串）或一个标签列表
 attrs 接收一个属性（字符串）或一个属性列表
le = LinkExtractor(tags=&#39;img&#39;, attrs=&#39;src&#39;)  process_value 接收一个形如func(value)的回调函数。如果传递了该参数，LinkExtractor将调用该回调函数对提取的每一个链接（如a的href）进行处理， 回调函数正常情况下应返回一个字符串（处理结果），想要抛弃所处理的链接时，返回None。</description>
    </item>
    
  </channel>
</rss>